results_df = pd.DataFrame(results).T
print("\nModel Performance Comparison:")
print(results_df)

plt.figure(figsize=(10, 8))
for name, (fpr, tpr) in roc_data.items():
    plt.plot(fpr, tpr, label=f'{name} (AUC = {results[name]["ROC AUC"]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison')
plt.legend()
plt.show()

plt.figure(figsize=(10, 8))
for name, (precision_curve, recall_curve) in pr_data.items():
    plt.plot(recall_curve, precision_curve, 
             label=f'{name} (AP = {results[name]["Avg Precision"]:.2f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve Comparison')
plt.legend()
plt.show()

tree_models = {
    'Decision Tree': models['Decision Tree'],
    'Random Forest': models['Random Forest'],
    'Gradient Boosting': models['Gradient Boosting']
}

plt.figure(figsize=(15, 5))
for i, (name, model) in enumerate(tree_models.items(), 1):
    if hasattr(model, 'feature_importances_'):
        plt.subplot(1, 3, i)
        importances = model.feature_importances_
        indices = np.argsort(importances)[::-1]
        plt.title(f'{name} Feature Importance')
        plt.barh(range(X.shape[1]), importances[indices], align='center')
        plt.yticks(range(X.shape[1]), X.columns[indices])
        plt.xlabel('Relative Importance')

plt.tight_layout()
plt.show()

metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']
plt.figure(figsize=(15, 8))
for i, metric in enumerate(metrics, 1):
    plt.subplot(2, 3, i)
    results_df[metric].plot(kind='bar')
    plt.title(metric)
    plt.xticks(rotation=45)
    plt.ylim(0, 1)

plt.tight_layout()
plt.show()
